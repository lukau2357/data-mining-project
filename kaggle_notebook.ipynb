{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training encoder-decoder models, an irritating deprecation warning is present, and I did not find a way to resolve it, other than downgrading to an older version of transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip uninstall transformers --y && pip install transformers==4.45.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import langid\n",
    "import langcodes\n",
    "import json\n",
    "import yaml\n",
    "import optuna\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer, AutoModelForSeq2SeqLM,\\\n",
    "                         Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq, DataCollatorWithPadding, TrainerCallback\n",
    "from transformers.modelcard import parse_log_history\n",
    "from typing import List\n",
    "from peft import LoraConfig, get_peft_model, PeftConfig, PeftModel\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# You should configure your HuggingFace token as a notebook secret.\n",
    "# https://www.kaggle.com/discussions/product-feedback/114053\n",
    "\n",
    "# Alternatively, if running locally, load from secrets.yaml, or simply copy/paste the value into the HF_TOKEN field\n",
    "if os.path.exists(\"secrets.yaml\"):\n",
    "    with open(\"secrets.yaml\", \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "        os.environ[\"HF_TOKEN\"] = data[\"hf_token\"]\n",
    "\n",
    "else:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    \n",
    "    # Global variables/parameters\n",
    "    HF_TOKEN = user_secrets.get_secret(\"hugging-face-access-token\")\n",
    "    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "SEED = 41\n",
    "TRAIN_FRAC = 0.8\n",
    "# Running on Kaggle\n",
    "# DATA_DIR = os.path.join(\"/\", \"kaggle\", \"input\", \"hate-speech-detection-curated-dataset\")\n",
    "# Running locally\n",
    "DATA_DIR = \"data\"\n",
    "DATASET_NAME = \"HateSpeechDatasetBalanced.csv\"\n",
    "DATA_PATH = os.path.join(DATA_DIR, DATASET_NAME)\n",
    "# Running on Kagge\n",
    "# RESULTS_DIR = os.path.join(\"/\", \"kaggle\", \"working\")\n",
    "# Running locally\n",
    "RESULTS_DIR = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "For model training, we will use **HateSpeechDatasetBalanced**, which is the augmented variant of the original dataset, with practically equal class distribution which is very convenient. We will perform a simple 80/20 train/eval split, although this is configurable as well. When it comes to data transformation itself, we will not do any preprocessing, the augmented dataset is already preprocessed, albeit lowercased as well, and one hypothesis is that for sentiment classification casing plays an important role as well. However, we cannot revert a lowercased sentence reliably back to it's original non-lowercased form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 train samples: 291620 1 train samples: 289275 Total train samples: 580895\n",
      "0 ratio: 0.5020 1 ratio: 0.4980\n",
      "0 test samples: 72905 1 test samples: 72319 Total test samples: 145224\n",
      "0 ratio: 0.5020 1 ratio: 0.4980\n"
     ]
    }
   ],
   "source": [
    "def tt_split(dataset_path : str, q : float, output_dir : str, seed : int, verbose : bool = True):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    df = df[[\"Content\", \"Label\"]]\n",
    "    # Some rows contain invalid values for Label column, disregard such rows\n",
    "    df = df[df[\"Label\"] != \"Label\"]\n",
    "    y = df[\"Label\"].to_numpy()\n",
    "\n",
    "    # Preserve class distribution in obtained splits.\n",
    "    X_train, X_test, _, _ = train_test_split(df, y, train_size = q, random_state = seed, stratify = y)\n",
    "\n",
    "    train_label_dis = X_train[\"Label\"].value_counts()\n",
    "    test_label_dis = X_test[\"Label\"].value_counts()\n",
    "\n",
    "    train_dis_0 = train_label_dis.iloc[0]\n",
    "    train_dis_1 = train_label_dis.iloc[1]\n",
    "    train_total = train_dis_0 + train_dis_1\n",
    "\n",
    "    test_dis_0 = test_label_dis.iloc[0]\n",
    "    test_dis_1 = test_label_dis.iloc[1]\n",
    "    test_total = test_dis_0 + test_dis_1\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"0 train samples: {train_dis_0} 1 train samples: {train_dis_1} Total train samples: {train_total}\")\n",
    "        print(f\"0 ratio: {(train_dis_0 / train_total):.4f} 1 ratio: {(train_dis_1 / train_total):.4f}\")\n",
    "    \n",
    "        print(f\"0 test samples: {test_dis_0} 1 test samples: {test_dis_1} Total test samples: {test_total}\")\n",
    "        print(f\"0 ratio: {(test_dis_0 / test_total):.4f} 1 ratio: {(test_dis_1 / test_total):.4f}\")\n",
    "\n",
    "    # filename = os.path.basename(dataset_path).split(\".\")[0]\n",
    "    # X_train.to_csv(os.path.join(output_dir, filename + \"_train.csv\"), index = False)\n",
    "    # X_test.to_csv(os.path.join(output_dir, filename + \"_test.csv\"), index = False)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "df_train, df_test = tt_split(DATA_PATH, TRAIN_FRAC, DATA_DIR, SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "The augmented dataset is well-formatted, in a sense that for every sentence every word is seperated by a whitespace. This allows us to measure the word length distribution over the given dataset with. Additionally, during manual inspection, I noticed that there are multiple languages present in the given dataset, so I was considering using a multilingual model. However, it turns out that the number of non-English sentences is insignificant to the number of English sentences, and multilingual models are much more complex than monolingual models in terms of parameter complexity, so I decided to use a monolingual model. \n",
    "\n",
    "I used the `langid` Python library to detect the language of the given sentece, the classifier itself is not perfect but it helps us to get a rough idea about the underlying language distrubiton. It also produced some false negatives - English sentences classified as non-English sentences, I suspect that this might also be the effect of data augmentation, so in the end I did not discard any sentences, even if they were classified as non-English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T11:52:39.112957Z",
     "iopub.status.busy": "2025-04-05T11:52:39.112186Z",
     "iopub.status.idle": "2025-04-05T12:10:26.505899Z",
     "shell.execute_reply": "2025-04-05T12:10:26.504724Z",
     "shell.execute_reply.started": "2025-04-05T11:52:39.112916Z"
    }
   },
   "outputs": [],
   "source": [
    "def identify_language(sentence : str):\n",
    "    return langcodes.Language.get((langid.classify(sentence))[0]).display_name()\n",
    "\n",
    "def sentence_word_length(sentence : str):\n",
    "    return len(sentence.split(\" \"))\n",
    "\n",
    "def word_length_distribution(dataset_path : str):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(\"Calculating sentence length over the given dataset.\")\n",
    "    df[\"WordLength\"] = df[\"Content\"].progress_apply(sentence_word_length)\n",
    "    print(\"Done with sentence length calculation.\")\n",
    "\n",
    "    value_counts = df[\"WordLength\"].value_counts().to_dict()\n",
    "    lengths = []\n",
    "\n",
    "    for key, value in value_counts.items():\n",
    "        lengths = lengths + [key] * value\n",
    "\n",
    "    print(\"Creating the sentence length distribution plot:\")\n",
    "    kde = gaussian_kde(lengths)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.set_xlabel(\"Comment length\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    ax.set_title(\"Comment length distribution\")\n",
    "\n",
    "    x = np.linspace(min(lengths), max(lengths), 1000)\n",
    "    bin_values = np.array(list(range(0, 301, 30)))\n",
    "    ax.plot(x, kde(x), label = \"Estimated PDF\")\n",
    "    ax.hist(lengths, alpha = 0.6, edgecolor = \"black\", density = True, bins = bin_values)\n",
    "    ax.legend()\n",
    "    ax.set_xticks(bin_values)\n",
    "    plt.show()\n",
    "    # fig_file = os.path.join(figures_dir, \"sentence_length_distribution.png\")\n",
    "    # fig.savefig(fig_file, bbox_inches = \"tight\")\n",
    "    # print(\"Sentence length distribution plot created and saved.\")\n",
    "\n",
    "def transform_language_counts(value_counts, threshold : float = 0.1):\n",
    "    total = sum(v for v in value_counts.values())\n",
    "    new_dict = {}\n",
    "\n",
    "    for key, value in value_counts.items():\n",
    "        if value >= int(total * threshold):\n",
    "            new_dict[key] = value\n",
    "        \n",
    "        else:\n",
    "            new_dict[\"Other\"] = new_dict.get(\"Other\", 0) + value\n",
    "    \n",
    "    return new_dict, total\n",
    "\n",
    "def language_distribution(dataset_path : str, threshold : float = 0.1):\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    print(\"Performing language classification over the given dataset:\")\n",
    "    df[\"Language\"] = df[\"Content\"].progress_apply(identify_language)\n",
    "    print(\"Done with language classification.\")\n",
    "    \n",
    "    value_counts = df[\"Language\"].value_counts().to_dict()\n",
    "    value_counts, total = transform_language_counts(value_counts, threshold)\n",
    "\n",
    "    print(\"Creating the language distribution plot:\")\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    bars = ax.barh(list(value_counts.keys()), value_counts.values(), edgecolor = \"black\")\n",
    "    ax.set_xlabel(\"Language\")\n",
    "    ax.set_ylabel(\"Number of comments\")\n",
    "    ax.set_title(\"Language distribution\")\n",
    "\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        y_position = bar.get_y() + bar.get_height() / 2  # Positioning text vertically centered on the bar\n",
    "\n",
    "        # Add number on top of the bar\n",
    "        ax.text(width, y_position, f\"{(width / total) * 100:.2f}%\", ha = \"left\", va = \"center\", fontsize = 10, color = \"black\")\n",
    "\n",
    "        # Draw a line from the label to the bar\n",
    "        ax.plot([width, width + 0.2], [y_position, y_position], color = \"black\", linewidth = 0.7)\n",
    "    plt.show()\n",
    "    # plt.savefig(os.path.join(figures_dir, \"language_distribution.png\"), bbox_inches = \"tight\")\n",
    "\n",
    "word_length_distribution(DATA_PATH)\n",
    "language_distribution(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning an encoder model\n",
    "For encoder models I've mostly experimented with RoBERTa encoders (https://huggingface.co/FacebookAI/roberta-base), but feel free to try different models. If you cannot afford long training times, I suggest taking a look at distilled variants, for RoBERTa - https://huggingface.co/distilbert/distilroberta-base. We will not fine-tune the full model, instead just a LoRA, with a configurable rank parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_URL = \"roberta-base\"\n",
    "CLASSIFIER_DROPOUT = 0.1\n",
    "LORA_DROPOUT = 0.1\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 5e-3\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "GRAD_ACCUM = 4\n",
    "LORA_RANK = 32\n",
    "LORA_ALPHA = LORA_RANK\n",
    "# Running on Kaggle\n",
    "# CHECKPOINT_DIR = os.path.join(\"/\", \"kaggle\", \"working\", \"encoder_checkpoints\", \"roberta_lora\")\n",
    "# Running locally\n",
    "CHECKPOINT_DIR = os.path.join(\"encoder_checkpoints\", \"roberta_base_alllora_r32_bs32_rslora\")\n",
    "FROM_CHECKPOINT = False # Set to True if you want to continue training from a checkpoint, otherwise set to None. False maybe works even?\n",
    "TRAIN_EPOCHS = 8\n",
    "WARMUP_RATIO = 0.1\n",
    "DATA_RATIO = 1 # Load only portion of the dataset for debugging, should be set to 1 once debugging is done.\n",
    "DISABLE_TQDM = False # I've had issues with TQDM rendering inside a notebook\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Dataset API integration\n",
    "We will use HuggingFace Dataset API to implement our dataset. Additionally, we will use the F1 metric to evaluate trained models. Even though in my opinion ROC-AUC metric provides is a more robust metric for comparing classification models because it incorporates different confidence thresholds, encoder-decoder models' output probability distributions cannot be easily converted to classwise probability distribution, so we can only rely on macro-average F1 for them.\n",
    "\n",
    "One possible optimization is to pre-tokenize the entire dataset, since tokenization is deterministic, and does not need to be repeated for every batch - we use Dataset.map function for that. Alternatively, you can save the result of pre-tokenization to a file and load sequences from the given file by using cache_file_name option, which is commented out currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4de2a88c8c994607b7b11727bfda5146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing the train dataset.:   0%|          | 0/580 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c9e1ad92c14af6a4bfeedd60fde110",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing the test dataset.:   0%|          | 0/145 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(batch, tokenizer):\n",
    "    # model maximum length is encoded in the pre-trained tokenizer.\n",
    "    inputs = tokenizer(batch[\"Content\"], truncation = True, padding = False, return_attention_mask = False)\n",
    "    inputs[\"labels\"] = batch[\"Label\"]\n",
    "    return inputs\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_URL, legacy = False)\n",
    "# https://huggingface.co/docs/datasets/v3.5.0/en/package_reference/main_classes#datasets.Dataset.from_pandas\n",
    "# https://huggingface.co/docs/datasets/v3.5.0/en/package_reference/main_classes#datasets.Dataset.map\n",
    "train_dataset = Dataset.from_pandas(df_train[:int(len(df_train) * DATA_RATIO)])\n",
    "train_dataset = train_dataset.map(partial(tokenize_function, tokenizer = tokenizer), \n",
    "                                  batched = True, \n",
    "                                  remove_columns = train_dataset.column_names,\n",
    "                                  desc = \"Tokenizing the train dataset.\",\n",
    "                                  keep_in_memory = True,\n",
    "                                  # cache_file_name = \"./train.cache\",\n",
    "                                  # load_from_cache_file = True\n",
    "                                 )\n",
    "\n",
    "test_dataset = Dataset.from_pandas(df_test[:int(len(df_test) * DATA_RATIO)])\n",
    "test_dataset = test_dataset.map(partial(tokenize_function, tokenizer = tokenizer), \n",
    "                                  batched = True, \n",
    "                                  remove_columns = test_dataset.column_names,\n",
    "                                  desc = \"Tokenizing the test dataset.\",\n",
    "                                  keep_in_memory = True,\n",
    "                                  # cache_file_name = \"./test.cache\",\n",
    "                                  # load_from_cache_file = True\n",
    "                                 )\n",
    "\n",
    "def compute_f1(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis = -1)\n",
    "    # This is equivalent to macro-averaged F1 since our dataset is well-balanced.\n",
    "    f1 = f1_score(labels, preds, average = \"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"eval_f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"Non-hateful\", 1: \"Hateful\"}\n",
    "label2id = {\"Hateful\": 1, \"Non-hateful\": 0}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_URL,\n",
    "    num_labels = 2,\n",
    "    classifier_dropout = CLASSIFIER_DROPOUT,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id,\n",
    "    token = False\n",
    ").to(DEVICE)\n",
    "\n",
    "trainable_model_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = LORA_RANK,\n",
    "    lora_dropout = LORA_DROPOUT,\n",
    "    bias = \"none\",\n",
    "    lora_alpha = LORA_ALPHA,\n",
    "    task_type = \"SEQ_CLS\",\n",
    "    target_modules = [\"query\", \"key\", \"value\", \"dense\"], # By default only key and value linear maps lare lora-ed, this adds more expressivity\n",
    "    # at the cost of model complexity.\n",
    "    layers_to_transform = list(range(12)), # Apply loras on every layer of the base model, except the classification head.\n",
    "    use_rslora = True\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "lora_trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Number of model trainable parameters: {trainable_model_params}\")\n",
    "print(f\"Number of LoRA trainable parameters: {lora_trainable_parameters}\")\n",
    "print(f\"Comperssion ratio: {trainable_model_params / lora_trainable_parameters:.4f}\")\n",
    "\n",
    "# Only applies within-batch sequence padding, to the length of the longest sequence.\n",
    "# We did not perform padding in Dataset.map due to redundancy, we are only storing \"important\" tokens!\n",
    "collator = DataCollatorWithPadding(tokenizer = tokenizer, padding = \"longest\")\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir = CHECKPOINT_DIR,\n",
    "    overwrite_output_dir = True,\n",
    "    fp16 = False,\n",
    "    report_to = \"none\",\n",
    "    gradient_accumulation_steps = GRAD_ACCUM,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    seed = SEED,\n",
    "    data_seed = SEED,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    weight_decay = WEIGHT_DECAY,\n",
    "    save_strategy = \"epoch\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    num_train_epochs = TRAIN_EPOCHS,\n",
    "    save_total_limit = 2,\n",
    "    load_best_model_at_end = True,\n",
    "    greater_is_better = True,\n",
    "    metric_for_best_model = \"eval_f1\",\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio = WARMUP_RATIO,\n",
    "    disable_tqdm = DISABLE_TQDM,\n",
    "    ddp_find_unused_parameters = False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = trainer_args,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    compute_metrics = compute_f1,\n",
    "    data_collator = collator\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint = FROM_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model will be loaded at the end, so you can perform inference immediately after training is finished. Alternatively, if you want to load a trained LoRA, you can use the template given below.\n",
    "\n",
    "**Add segment where you put a URL to a model that you trained, and load it here and demonstrate that it works**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = os.path.join(\"encoder_checkpoints\", \"roberta_base_alllora_r8_rslora\", \"checkpoint-290448\")\n",
    "lora_config = PeftConfig.from_pretrained(checkpoint_path)\n",
    "id2label = {0: \"Non-hateful\", 1: \"Hateful\"}\n",
    "label2id = {\"Hateful\": 1, \"Non-hateful\": 0}\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(lora_config.base_model_name_or_path, id2label = id2label, label2id = label2id).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(lora_config.base_model_name_or_path)\n",
    "model = PeftModel.from_pretrained(base_model, checkpoint_path).to(DEVICE) # Will use a single GPU for inference, but that is fine since no gradients are computed.\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    sentences = [\"Thank you very much.\", \"I hate you!\"]\n",
    "    enc = tokenizer(sentences, padding = True, truncation = True, return_tensors = \"pt\").to(DEVICE)\n",
    "    res = model(**enc)\n",
    "\n",
    "    logits = res.logits.cpu()\n",
    "    probs = torch.nn.functional.softmax(logits, dim = -1)\n",
    "    preds = torch.argmax(probs, dim = -1)\n",
    "    counter = 0\n",
    "\n",
    "    for pred, sentence in zip(preds, sentences):\n",
    "        print(f\"{sentence} Prediction: {model.config.id2label[int(pred.item())]} Probabilities: {probs[counter][0]:.4f} {probs[counter][1]:.4f}\")\n",
    "        counter += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning an encoder-decoder model\n",
    "Now we will try to utilize a full encoder-decoder architecture to see if we can improve previous results. One of the most popular encoder-decoder models is **T5 - Text to Text Transfer Transformer** (https://arxiv.org/pdf/1910.10683). On a high level, it transforms all NLP tasks into text2text problems, and then solves them using an encoder/decoder architecture. A high level overview of this paradigm can be seen of the figure below:\n",
    "\n",
    "![no figure](https://miro.medium.com/v2/resize:fit:791/1*QOVXAn0bx8HKGrBIXAgydw.png)\n",
    "\n",
    "We will take a pretrained T5 model, and fine-tune it for text classification. For this setting, our encoder will take the sentence in question, and the decoder will be fine-tuned to autoregressively predict the correct class. Class labels that we choose are also hyperparameters, but I do not think they can effect results that much if the model is trained for sufficiently long, provided that they are semantically consistent with actual labels of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_URL = \"google/flan-t5-base\"\n",
    "LORA_DROPOUT = 0.1\n",
    "LORA_ALPHA = 18\n",
    "LEARNING_RATE = 3e-4 # Authors of FLAN T5 use higher learning rate compared to that of RoBERTa, we will adhere to that.\n",
    "WEIGHT_DECAY = 1e-3\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM = 2\n",
    "LORA_RANK = 18\n",
    "# Running on Kaggle\n",
    "# CHECKPOINT_DIR = os.path.join(\"/\", \"kaggle\", \"working\", \"encoder_decoder_checkpoints\", \"flan_t5_small_test\")\n",
    "# Running locally\n",
    "CHECKPOINT_DIR = os.path.join(\"encoder_decoder_checkpoints\", \"flan_t5_base_r18_e8\")\n",
    "FROM_CHECKPOINT = True # Set to True if you want to continue training from a checkpoint, otherwise set to None. False maybe works even?\n",
    "TRAIN_EPOCHS = 8\n",
    "WARMUP_RATIO = 0.1\n",
    "DATA_RATIO = 1 # Load only portion of the dataset for debugging, should be set to 1 once debugging is done.\n",
    "DISABLE_TQDM = False # I've had issues with TQDM rendering inside a notebook\n",
    "HATE_LABEL = \"Hateful\"\n",
    "NON_HATE_LABEL = \"Non-hateful\"\n",
    "\n",
    "LABEL2TEXT = {\n",
    "    0: NON_HATE_LABEL,\n",
    "    1: HATE_LABEL\n",
    "}\n",
    "\n",
    "TEXT2LABEL = {\n",
    "    v: k for k, v in LABEL2TEXT.items()\n",
    "}\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PREFIX = f\"Classify this sentence as {HATE_LABEL} or {NON_HATE_LABEL} speech:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-decoder dataset class\n",
    "Pre-processing for encoder-decoder models is more complex - target sequences need to be shifted accordingly. Instead of using PyTorch Dataset, we will use HuggingFace Dataset API, along with DataCollatorForSeq2Seq. With these 2 classes, we will delegate the entire tokenization procedure to HuggingFace. Additionally, HuggingFace Dataset will allow us to easily pre-tokenize the entire dataset, which was an issue with previous encoder approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f07c7dccb24599aa42e750f5dd80b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing the train dataset.:   0%|          | 0/580895 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92c393f6b6b4aeea4c2665414429f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing the test dataset.:   0%|          | 0/145224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(batch, tokenizer, prefix = \"\"):\n",
    "    # model maximum length is encoded in the pre-trained tokenizer.\n",
    "    content = [f\"{prefix} {x}\".strip() for x in batch[\"Content\"]]\n",
    "    inputs = tokenizer(content, truncation = True, padding = False)\n",
    "    targets = tokenizer(text_target = [LABEL2TEXT[int(x)] for x in batch[\"Label\"]], truncation = True, padding = False)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_URL, legacy = False)\n",
    "# https://huggingface.co/docs/datasets/v3.5.0/en/package_reference/main_classes#datasets.Dataset.from_pandas\n",
    "# https://huggingface.co/docs/datasets/v3.5.0/en/package_reference/main_classes#datasets.Dataset.map\n",
    "train_dataset = Dataset.from_pandas(df_train[:int(len(df_train) * DATA_RATIO)])\n",
    "train_dataset = train_dataset.map(partial(tokenize_function, tokenizer = tokenizer, prefix = PREFIX), \n",
    "                                  batched = True, \n",
    "                                  remove_columns = train_dataset.column_names,\n",
    "                                  desc = \"Tokenizing the train dataset.\",\n",
    "                                  keep_in_memory = True\n",
    "                                  # cache_file_name = os.path.join(\"/\", \"kaggle\", \"working\", \"train_seq2seq.cache\"),\n",
    "                                  # load_from_cache_file = True\n",
    "                                 )\n",
    "\n",
    "test_dataset = Dataset.from_pandas(df_test[:int(len(df_test) * DATA_RATIO)])\n",
    "test_dataset = test_dataset.map(partial(tokenize_function, tokenizer = tokenizer, prefix = PREFIX), \n",
    "                                  batched = True, \n",
    "                                  remove_columns = test_dataset.column_names,\n",
    "                                  desc = \"Tokenizing the test dataset.\",\n",
    "                                  keep_in_memory = True\n",
    "                                  # cache_file_name = os.path.join(\"/\", \"kaggle\", \"working\", \"test_seq2seq.cache\"),\n",
    "                                  # load_from_cache_file = True\n",
    "                                )\n",
    "\n",
    "target_encodings = tokenizer(text_target = [v for v in LABEL2TEXT.values()], padding = True, return_tensors = \"pt\")\n",
    "MAX_TARGET_LEN = target_encodings[\"input_ids\"].shape[1]\n",
    "\n",
    "PAD_ID = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "def compute_f1_seq2seq(pred, tokenizer):\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens = True)\n",
    "    # Labels are -100 for pad tokens, so replace them with actual pad id.\n",
    "    # https://huggingface.co/docs/transformers/en/model_doc/t5?usage=AutoModel#transformers.T5ForConditionalGeneration.forward.labels\n",
    "    labels[labels == -100] = PAD_ID\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens = True)\n",
    "\n",
    "    decoded_labels = [TEXT2LABEL[item] for item in decoded_labels]\n",
    "    decoded_preds = [TEXT2LABEL.get(item.strip(), -1) for item in decoded_preds]\n",
    "\n",
    "    # Class -1 captures the case when model output does not match to any of the classes, we create a new class for that instance.\n",
    "    # Since decoded_preds != -1 always, the number of instances having class -1 is always 0, so we explicitly set zero_divison = 0\n",
    "    # so that our final result is not affected by this.\n",
    "    f1 = f1_score(decoded_preds, decoded_labels, labels = [0, 1, -1], average = \"weighted\", zero_division = 0)\n",
    "    \n",
    "    return {\n",
    "        \"eval_f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional base model evaluation\n",
    "Optionally, you can evaluate the performance of a baseline T5 model. A model like FLAN-T5 which we will be using which was fine-tuned already should give reasonable results - I would expect above 0.6 F1. We will perform another iteration of fine-tuning over our dataset to further push the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_URL).to(DEVICE)\n",
    "predictions, ground_truth = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f\"Baseline model evaluation: {MODEL_URL}\")\n",
    "    batch_size = 128\n",
    "    \n",
    "    for i in tqdm_notebook(range(0, len(test_dataset), batch_size)):\n",
    "        batch = test_dataset[i : i + batch_size]\n",
    "        # Manual padding because of dataset construction required.\n",
    "        batch = tokenizer.pad(batch, padding = \"longest\")\n",
    "        input_ids = torch.tensor(batch[\"input_ids\"], device = DEVICE)\n",
    "        labels = tokenizer.batch_decode(batch[\"labels\"], skip_special_tokens = True)\n",
    "        \n",
    "        output = model.generate(inputs = input_ids, max_new_tokens = MAX_TARGET_LEN)\n",
    "        output = tokenizer.batch_decode(output, skip_special_tokens = True)\n",
    "        output = [item.strip() for item in output]\n",
    "\n",
    "        ground_truth += [TEXT2LABEL[item] for item in labels]\n",
    "        predictions += [TEXT2LABEL.get(item, -1) for item in output]\n",
    "\n",
    "# If the model is well-behaved, nothing will be generated outside Hateful and Non-hateful,  \n",
    "cls_rprt = classification_report(predictions, ground_truth, labels = [0, 1, -1], zero_division = 0)\n",
    "print(f\"Baseline classification report:\")\n",
    "print(cls_rprt)\n",
    "baseline_f1 = f1_score(predictions, ground_truth, labels = [0, 1], average = \"weighted\", zero_division = 0)\n",
    "print(f\"Baseline F1:\")\n",
    "print(baseline_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an encoder-decoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_URL).to(DEVICE)\n",
    "# generate calls will default to greedy decoding. The decoding algorithm can be viewed as a hyperparameter also!\n",
    "# https://huggingface.co/docs/transformers/v4.51.0/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "print(f\"Model generation config: {model.generation_config}\")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = LORA_RANK,\n",
    "    lora_dropout = LORA_DROPOUT,\n",
    "    bias = \"none\",\n",
    "    lora_alpha = LORA_ALPHA,\n",
    "    task_type = \"SEQ_2_SEQ_LM\",\n",
    "    # at the cost of model complexity.\n",
    "    layers_to_transform = list(range(12)), # Apply loras on every layer of the base model, except the classification head.\n",
    "    use_rslora = True\n",
    ")\n",
    "\n",
    "trainable_model_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Autoregressive generation is significantlly slower than a simple forward pass through the model\n",
    "# which explains why training is slower as well on a T5 model.\n",
    "#import time\n",
    "#x = train_dataset[:16]\n",
    "#x[\"input_ids\"] = torch.tensor(x[\"input_ids\"]).to(DEVICE)\n",
    "#x[\"attention_mask\"] = torch.tensor(x[\"attention_mask\"]).to(DEVICE)\n",
    "#x[\"labels\"] = torch.tensor(x[\"labels\"]).to(DEVICE)\n",
    "#start = time.time()\n",
    "#y = model.generate(**x, max_length = MAX_TARGET_LEN)\n",
    "# y = model(**x)\n",
    "#end = time.time()\n",
    "#print(f\"Generation time: {end - start}:.4f seconds.\")\n",
    "#print(y)\n",
    "#return\n",
    "\n",
    "lora_trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# 124647170 - Number of RoBERTa-base parameters\n",
    "# 247577856 - Number of T5 base parameters. Roughly 2 times higher than RoBERTa, which is expected.\n",
    "print(f\"Number of model trainable parameters: {trainable_model_params}\")\n",
    "print(f\"Number of LoRA trainable parameters: {lora_trainable_parameters}\")\n",
    "print(f\"Compression ratio: {trainable_model_params / lora_trainable_parameters:.4f}\")\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer = tokenizer, model = model, padding = \"longest\")\n",
    "\n",
    "# Control predict_with_generate behaviour:\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments.predict_with_generate\n",
    "# https://huggingface.co/docs/transformers/v4.51.0/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "# https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.generate\n",
    "# Defaults to greedy decoding, this can also be seen as a training hyperparameter!\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir = CHECKPOINT_DIR,\n",
    "    overwrite_output_dir = True,\n",
    "    fp16 = False,\n",
    "    report_to = \"none\",\n",
    "    gradient_accumulation_steps = GRAD_ACCUM,\n",
    "    per_device_train_batch_size = BATCH_SIZE,\n",
    "    seed = SEED,\n",
    "    data_seed = SEED,\n",
    "    learning_rate = LEARNING_RATE,\n",
    "    weight_decay = WEIGHT_DECAY,\n",
    "    save_strategy = \"epoch\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    num_train_epochs = TRAIN_EPOCHS,\n",
    "    save_total_limit = 2,\n",
    "    load_best_model_at_end = True,\n",
    "    greater_is_better = True,\n",
    "    metric_for_best_model = \"eval_f1\",\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    warmup_ratio = WARMUP_RATIO,\n",
    "    disable_tqdm = DISABLE_TQDM,\n",
    "    ddp_find_unused_parameters = False,\n",
    "    predict_with_generate = True,\n",
    "    generation_config = MODEL_URL,\n",
    "    generation_max_length = MAX_TARGET_LEN # Attempt to optimize generation a little bit...\n",
    ")\n",
    "\n",
    "# Can't solve Trainer.tokenizer is deprecated warning...\n",
    "# https://github.com/hiyouga/LLaMA-Factory/issues/6130\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    data_collator = collator,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    compute_metrics = partial(compute_f1_seq2seq, tokenizer = tokenizer)\n",
    ")\n",
    "trainer.train(resume_from_checkpoint = FROM_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Hyperparameter optimization\n",
    "With this setup, RoBERTa achieves better F1 than FLAN-T5. Although results for FLAN-T5 could probably be improved by additional hyperparameter optimzation, training time for FLAN-T5 is significantly longer than training time for RoBERTa, due to the former being a full encoder-decoder architecture, we require autoregressive decoding during evaluation.\n",
    "\n",
    "We've decided to perform a more extensive hyperparameter search on RoBERTa, using **optuna**. The main bottleneck for this particular problem is the size of the dataset - $\\approx 500 \\times 10^3$ sentences in the training set, so we can expect relatively long training times even with lower LoRA rank values. Therefore, we perform hyperparameter optimization only on the portion of the original dataset. In particular, we take a 30% stratified sample of the original dataset, and from there construct train/validation datasets with the train dataset having 80% of the previously sampled rows - this amounts to training on roughly a quarter of the original dataset. These values are configurable in the cell below, as well as the maximum and minimum learning rate and weight decay values during HPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum and maximum learning rate\n",
    "LR_MIN = 1e-6\n",
    "LR_MAX = 1\n",
    "\n",
    "# Minimum and maximum weight decay\n",
    "WD_MIN = 1e-5\n",
    "WD_MAX = 1e-1\n",
    "\n",
    "HPO_TRAIN_EPOCHS = 4\n",
    "HPO_DATA_RATIO = 0.25\n",
    "HPO_TRAIN_FRAC = 0.8\n",
    "WARMUP_RATIO = 0.1\n",
    "# Number of HPO rounds to perform. Generally greater is better.\n",
    "HPO_TRIALS = 10\n",
    "# Number of epochs to perform during single HPO instance\n",
    "HPO_EPOCHS = 8\n",
    "\n",
    "MODEL_URL = \"roberta-base\"\n",
    "CLASSIFIER_DROPOUT = 0.1\n",
    "LORA_DROPOUT = 0.1\n",
    "\n",
    "# Could also optimize over batch size, decided to keep it fixed to 32.\n",
    "BATCH_SIZE = 8\n",
    "GRAD_ACCUM = 4\n",
    "\n",
    "# Kept LoRA rank fixed to 32, generally in big data regime bigger transformer models will outperform smaller ones.\n",
    "# Potentially calibrate this parameter as well.\n",
    "LORA_RANK = 32\n",
    "LORA_ALPHA = LORA_RANK\n",
    "\n",
    "DISABLE_TQDM = False # I've had issues with TQDM rendering inside a notebook\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "id2label = {0: \"Non-hateful\", 1: \"Hateful\"}\n",
    "label2id = {\"Hateful\": 1, \"Non-hateful\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "1    145\n",
      "0    145\n",
      "Name: count, dtype: int64\n",
      "Label\n",
      "1    37\n",
      "0    36\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "hpo_dataset, _ = tt_split(DATA_PATH, HPO_DATA_RATIO, DATA_DIR, SEED, verbose = False)\n",
    "hpo_train_df, hpo_test_df, _, _ = train_test_split(hpo_dataset, hpo_dataset[\"Label\"], \n",
    "                                                  train_size = HPO_TRAIN_FRAC, \n",
    "                                                  random_state = SEED, \n",
    "                                                  stratify = hpo_dataset[\"Label\"])\n",
    "\n",
    "# Verify that HPO train and test datasets preserve original class distribution.\n",
    "print(hpo_train_df[\"Label\"].value_counts())\n",
    "print(hpo_test_df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same dataset structure as with initial RoBERTa model. Code is exactly the same, except for using different data frames for dataset construction. Code repetition could have been avoided with a more thoughtful code parameterisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(batch, tokenizer):\n",
    "    # model maximum length is encoded in the pre-trained tokenizer.\n",
    "    inputs = tokenizer(batch[\"Content\"], truncation = True, padding = False, return_attention_mask = False)\n",
    "    inputs[\"labels\"] = batch[\"Label\"]\n",
    "    return inputs\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_URL, legacy = False)\n",
    "# https://huggingface.co/docs/datasets/v3.5.0/en/package_reference/main_classes#datasets.Dataset.from_pandas\n",
    "# https://huggingface.co/docs/datasets/v3.5.0/en/package_reference/main_classes#datasets.Dataset.map\n",
    "train_dataset = Dataset.from_pandas(hpo_train_df)\n",
    "train_dataset = train_dataset.map(partial(tokenize_function, tokenizer = tokenizer), \n",
    "                                  batched = True, \n",
    "                                  remove_columns = train_dataset.column_names,\n",
    "                                  desc = \"Tokenizing the train dataset.\",\n",
    "                                  keep_in_memory = True,\n",
    "                                  # cache_file_name = \"./train.cache\",\n",
    "                                  # load_from_cache_file = True\n",
    "                                 )\n",
    "\n",
    "test_dataset = Dataset.from_pandas(hpo_test_df)\n",
    "test_dataset = test_dataset.map(partial(tokenize_function, tokenizer = tokenizer), \n",
    "                                  batched = True, \n",
    "                                  remove_columns = test_dataset.column_names,\n",
    "                                  desc = \"Tokenizing the test dataset.\",\n",
    "                                  keep_in_memory = True,\n",
    "                                  # cache_file_name = \"./test.cache\",\n",
    "                                  # load_from_cache_file = True\n",
    "                                 )\n",
    "\n",
    "def compute_f1(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis = -1)\n",
    "    # This is equivalent to macro-averaged F1 since our dataset is well-balanced.\n",
    "    f1 = f1_score(labels, preds, average = \"weighted\")\n",
    "\n",
    "    return {\n",
    "        \"eval_f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalCallback(TrainerCallback):\n",
    "    def __init__(self, trial):\n",
    "        self.trial = trial\n",
    "        \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        return\n",
    "        # Pruning seems to be too strict. Uncomment to enable prunning\n",
    "        # eval_f1 = kwargs[\"metrics\"][\"eval_f1\"]\n",
    "        # self.trial.report(eval_f1, step = state.epoch)\n",
    "        # if self.trial.should_prune():\n",
    "        #    raise optuna.TrialPruned()\n",
    "\n",
    "def model_init():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_URL, \n",
    "    num_labels = 2,\n",
    "    classifier_dropout = CLASSIFIER_DROPOUT,\n",
    "    id2label = id2label,\n",
    "    label2id = label2id).to(DEVICE)\n",
    "\n",
    "    trainable_model_params = sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r = LORA_RANK,\n",
    "        lora_dropout = LORA_DROPOUT,\n",
    "        bias = \"none\",\n",
    "        lora_alpha = LORA_ALPHA,\n",
    "        task_type = \"SEQ_CLS\",\n",
    "        target_modules = [\"query\", \"key\", \"value\", \"dense\"], # By default only key and value linear maps lare lora-ed, this adds more expressivity\n",
    "        # at the cost of model complexity.\n",
    "        layers_to_transform = list(range(12)), # Apply loras on every layer of the base model, except the classification head.\n",
    "        use_rslora = True\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, lora_config)\n",
    "    lora_trainable_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Number of model trainable parameters: {trainable_model_params}\")\n",
    "    print(f\"Number of LoRA trainable parameters: {lora_trainable_parameters}\")\n",
    "    print(f\"Comperssion ratio: {trainable_model_params / lora_trainable_parameters:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float('learning_rate', LR_MIN, LR_MAX, log = True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", WD_MIN, WD_MAX, log = True)\n",
    "    \n",
    "    trainer_args = TrainingArguments(\n",
    "        output_dir = None,\n",
    "        save_strategy = \"no\",\n",
    "        overwrite_output_dir = True,\n",
    "        fp16 = False,\n",
    "        report_to = \"none\",\n",
    "        gradient_accumulation_steps = GRAD_ACCUM,\n",
    "        seed = SEED,\n",
    "        data_seed = SEED,\n",
    "        # No eval strategy since it is performed manually\n",
    "        eval_strategy = \"epoch\",\n",
    "        logging_strategy = \"epoch\",\n",
    "        per_device_train_batch_size = BATCH_SIZE,\n",
    "        learning_rate = learning_rate,\n",
    "        weight_decay = weight_decay,\n",
    "        num_train_epochs = HPO_EPOCHS,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        metric_for_best_model = \"eval_f1\",\n",
    "        warmup_ratio = WARMUP_RATIO,\n",
    "        disable_tqdm = DISABLE_TQDM,\n",
    "        ddp_find_unused_parameters = False\n",
    "    )\n",
    "\n",
    "    collator = DataCollatorWithPadding(tokenizer = tokenizer, padding = \"longest\")\n",
    "    model = model_init()\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model = model,\n",
    "        data_collator = collator,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = test_dataset,\n",
    "        compute_metrics = compute_f1,\n",
    "        args = trainer_args\n",
    "    )\n",
    "    \n",
    "    trainer.add_callback(EvalCallback(trial))\n",
    "    trainer.train()\n",
    "    \n",
    "    # Extract validation loss from last evaluate and return that as HPO result for the current trial\n",
    "    parsed_history = parse_log_history(trainer.state.log_history)\n",
    "    res = float(\"-inf\")\n",
    "\n",
    "    for item in parsed_history[1]:\n",
    "        res = max(res, item.get(\"F1\", 0))\n",
    "    \n",
    "    return res\n",
    "\n",
    "# Checkpointing for Optuna HPO since it does take a while to complete.\n",
    "study_name = \"roberta-hpo\"\n",
    "storage_name = f\"sqlite:///{os.path.join(os.getcwd(), study_name + '.db')}\"\n",
    "study = optuna.create_study(storage = storage_name, study_name = study_name, direction = \"maximize\", load_if_exists = True)\n",
    "study.optimize(objective, n_trials = HPO_TRIALS)\n",
    "\n",
    "with open(os.path.join(os.getcwd(), study_name + '_best_params.json'), \"w+\") as f:\n",
    "    res = {\n",
    "        \"trial_number\": study.best_trial.number,\n",
    "        \"best_value\": study.best_value,\n",
    "        \"params\": study.best_params\n",
    "    }\n",
    "    json.dump(res, f)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4205998,
     "sourceId": 7257995,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
